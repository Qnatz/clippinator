# SerpAPI Key (Optional, for enabling search capabilities for agents via a general search tool)
# If no general search tool is used, or if its search capabilities are not needed, this can be omitted.
# SERPAPI_API_KEY=your_serpapi_api_key_here

# --- LlamaCpp Model Configuration ---
# These settings are for using a local GGUF model with LlamaCpp.
# Uncomment and set these variables to point to your model and configure its parameters.

# Path to your GGUF model file
# MODEL_PATH=./models/your_model_name.gguf
# Example for Deepseek Coder 1.3B Instruct:
# MODEL_PATH=./models/deepseek-coder-1.3b-instruct.Q4_K_M.gguf

# Number of GPU layers to offload. Set to 0 if not using GPU acceleration or unsure.
# Adjust based on your GPU's VRAM.
# N_GPU_LAYERS=0

# Batch size for prompt processing.
# N_BATCH=512

# Context size (max sequence length).
# N_CTX=2048

# LLM sampling temperature. Lower values make the output more deterministic.
# LLM_TEMPERATURE=0.1

# Maximum number of tokens the LLM should generate in a single response.
# LLM_MAX_TOKENS=1024
